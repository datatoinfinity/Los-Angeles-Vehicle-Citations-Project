---
title: "Los Angeles Vehicle Citations Project - Data Cleaning"
author: "Brandon Thoma"
date: "March 22, 2019"
output: html_document
---

# Import Libraries

```{r, message = FALSE, warning = FALSE}
x <- c("dplyr", "magrittr", "stringr", "stringi", "lubridate", "ggplot2",
       "scales", "proj4", "maps", "ggmap", "tidyr", "data.table", "gridExtra")
lapply(x, library, character.only = TRUE)
source("Custom Created Functions.R")
rm(x)
```

# Introduction - Import and Subsetting

```{r, message = FALSE, warning = FALSE}
# The Raw Citations Data can be found at
# https://www.kaggle.com/cityofLA/los-angeles-parking-citations. The data is 
# constantly updated and me be larger than the file I used at the time of this
# project.
raw_data <- fread("Raw Citations Data.csv", sep = ",")
# Remove non-useful columns (for this particular project analysis).
clean_data <- raw_data %>%
  select("Issue Date", "Issue time", "Make", "Location", "Violation code",
         "Violation Description", "Fine amount", "Latitude", "Longitude") %>%
  rename(Issue_Date = `Issue Date`, Issue_Time = `Issue time`,
         Violation_Code = `Violation code`, Fine_Amount = `Fine amount`,
         Violation_Description = `Violation Description`)
```

# Assumptions Going Forward

```{r}
# Tickets are unique, no duplicates. Assume for simplicity that this means 
# that no vehicle was cited more than once in a given instance. 
length(unique(clean_data$`Ticket number`)) == nrow(clean_data)
```

# Issue Date Cleaning

```{r, echo = FALSE}
# Cleanup Issue Date column by removing erroneous timestamp, leaving as
# character for later.
clean_data$Issue_Date <- gsub('T00:00:00', "", clean_data$Issue_Date) 
# Only will look at 2018 for this analysis. The following method is faster than
# subsetting and using format.
clean_data$Year <- year(ymd(clean_data$Issue_Date))
clean_data <- subset(clean_data, Year == 2018) %>%
  select(., select = -Year)
```

# Issue Time Cleaning

```{r}
# Issue Time for the ticket is originally stored in the raw data based on a 
# custom numeric value based on military time that indicates the hour/minutes
# since midnight that the ticket was issued. Cleaned the variable so it stores
# the hours and minutes in proper military time.
clean_data$Issue_Time <- as.character(clean_data$Issue_Time) %>%
  correct_time(clean_data, ., "Issue_Time")
```

# Issue Date-time (New Variable) Creation

```{r}
# New column that stores the date and time.
clean_data$Issue_DT <- apply(clean_data[, c('Issue_Date','Issue_Time')], 1,
                             paste, collapse = " ") %>%
# Some values in the raw data were missing Issue Time (NA), so were quieted to
# not show in output.
  ymd_hm(., quiet = TRUE)
# Change timezone from UTC.
tz(clean_data$Issue_DT) <- 'America/Los_Angeles'
# Now remove unnecessary columns.
clean_data <- clean_data %>%
  select(-c(Issue_Time, Issue_Date))
```

# Tickets by Month and Day, Day of the Week, and Time of Day

```{r}
clean_data$Month <- lubridate::month(clean_data$Issue_DT, label = TRUE)
clean_data$Date <- as.Date(clean_data$Issue_DT)
clean_data$Weekday <- weekdays(clean_data$Issue_DT)
# Need to standardize the date to one day to be able to plot citations by time
# of day.
clean_data$TOD <- strftime(clean_data$Issue_DT, format = "%H:%M:%S",
                           tz = "America/Los_Angeles") %>%
  as.POSIXct(., format = "%H:%M:%S")
```

# Latitude and Longitude Coordinate Cleaning

```{r}
# 11% of the 2018 data were missing Latitude and Longitude data (the values
# were stored as 99999). Removed these values for the mapping data.
coord_data <- clean_data %>%
  filter(Latitude != 99999 & Longitude != 99999)
# The coordinates in the raw data have latitude / longitude (X/Y) in US Feet
# coordinates according to the NAD_1983_StatePlane_California_V_FIPS_0405_Feet
# projection. We need to change translate these coordinates into a more
# standard format of coordinate to use for Google maps plotting.
# The specific parameters for proj4string is found based on
# https://epsg.io/transform#s_srs=102645&t_srs=4326 and
# https://epsg.io/transform#s_srs=102645&t_srs=4326 pages.
proj4string <- paste('+proj=lcc +lat_1=34.03333333333333',
                     ' +lat_2=35.46666666666667 +lat_0=33.5 +lon_0=-118 ',
                     '+x_0=2000000 +y_0=500000.0000000002 +ellps=GRS80 ',
                     '+datum=NAD83 +to_meter=0.3048006096012192 +no_defs')
# Translate the coordinates between coordinate systems so we can plot them.
projections <- project(coord_data[, c("Latitude", "Longitude")], proj4string,
                       inverse = TRUE)
new_coords <- data.frame(latitude = projections$y, longitude = projections$x)
coord_data$New_Latitude <- new_coords$latitude
coord_data$New_Longitude <- new_coords$longitude
cit_coords <- coord_data %>%
  group_by(New_Latitude, New_Longitude) %>%
  summarise(Frequency = n()) %>%
  rename(lat = New_Latitude, long = New_Longitude)
```

# Highest Average Fines by Area

```{r}
fines_area <- coord_data %>%
  select("Fine_Amount", "Latitude", "Longitude")  
avg_area_fines <- fines_area %>%
  group_by(Latitude, Longitude) %>%
  summarise(Avg_Fine = mean(Fine_Amount))
```

# Write Excel Spreadsheets

```{r}
# For date-time columns, will be converted with write.csv, which saves as a
# character for easier conversion back.
fwrite(clean_data, "Cleaned Data.csv", dateTimeAs = "write.csv")
fwrite(cit_coords, "Citations by Location.csv")
fwrite(avg_area_fines, "Average Fine by Location.csv")
```

# Extra Code

```{r}
# Total Fines by Month
tot_fines_by_month <- clean_cits_data %>%
  group_by(Month) %>%
  summarise(`Tot Month Fines` = sum(`Fine amount`, na.rm = TRUE)) %>%
  filter(!is.na(Month))
# No longer need Month column.
clean_cits_data <- subset(clean_cits_data, select = -Month)



clean_cits_data$`Marked Time` <- as.character(clean_cits_data$`Marked Time`)
clean_cits_data$`Marked Time` <- sapply(clean_cits_data$`Marked Time`,
                                        function(x) {
  if (!is.na(x)) {
    if (str_length(x) == 3) {
      y <- paste('0', x, sep = "")
      z <- strsplit(y, "")[[1]]
      x <- paste0(c(z[1:2], ":", z[3:4]), collapse = '')
    } else if (str_length(x) == 2) {
      x <- paste('00:', x, sep = "")
    } else if (str_length(x) == 1) {
      x <- paste('00:0', x, sep = "")
    } else {
      y <- strsplit(x, "")[[1]]
      x <- paste0(c(y[1:2], ":", y[3:4]), collapse = '')
    }
  } else {
      x <- x
  }
})

clean_cits_data$M_Datetime <- apply(
  clean_cits_data[ ,c('Issue Date','Marked Time')] , 1, paste, collapse = " ")
# The method with lubridate was much faster than the others I tried.
clean_cits_data$M_Datetime <- ymd_hm(clean_cits_data$M_Datetime, quiet = TRUE)
tz(clean_cits_data$M_Datetime) <- 'America/Los_Angeles'
# Now unnecessary column.
clean_cits_data <- clean_cits_data %>%
  select(-c(`Issue Date`, `Marked Time`))

a <- subset(clean_cits_data, !(is.na(clean_cits_data$M_Datetime)))
d <- a %>%
  group_by(TOD, Weekday) %>%
  summarise(`Cits By TOD` = n()) %>%
  filter(!is.na(TOD))

p <- ggplot(d, aes(x = TOD, y = `Cits By TOD`)) +
  geom_line(color = "red", size = 0.25) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        legend.position = "bottom") + 
  labs(x = 'Citation Time', y = "# of Citations") + 
  facet_wrap( ~ Weekday, ncol = 2, scales = "free") +
  scale_x_datetime(breaks = date_breaks("1 hour"),
                   labels = date_format("%H:%M", tz = "America/Los_Angeles"))
print(p)

over_time_park <- subset(clean_cits_data, !(is.na(clean_cits_data$M_Datetime)))
over_time_park$`Time Overparked` <- (over_time_park$I_Datetime -
                                     over_time_park$M_Datetime)

plot(over_time_park$`Time Overparked`)
# Remove outliers (top 5% of positive times) and negative times.
over_time_park <- subset(over_time_park, `Time Overparked` >= 0)
# `Time Overparked` is time-difference formatted, cast to numeric to compute.
cutoff <- min(boxplot(as.numeric(over_time_park$`Time Overparked`))$out)
over_time_park <- subset(over_time_park, `Time Overparked` < cutoff)
# `Time Overparked` originally in seconds, easier to view in minutes.
over_time_park$`Time Overparked` <- over_time_park$`Time Overparked` / 60
hist(as.numeric(over_time_park$`Time Overparked`), xlab = "Minutes Overparked",
     main = "Number of Minutes Overparked")

#cutoffs_1 <- c(as.Date("2019-04-01"), as.Date("2019-04-30"))
#april_cits <- subset(cits_by_day, (Date >= cutoffs_1[1] &
#                                   Date <= cutoffs_1[2]))
#graph_cits(april_cits, "Citations for April 2018", "Date", "Tot_Day_Cits")
#e <- ggplot(april_cits, aes(x = Date, y = Tot_Day_Cits)) +
#  geom_bar(color = "red", fill = "red", stat = "identity") + 
#  theme(axis.text.x = element_text(hjust = 1),
#        plot.title = element_text(hjust = 0.5), legend.position = "bottom") +
#  ggtitle("Citations for April 2018") +
#  labs(x = 'Month and Day', y = "# of Citations")
#print(e)

#e <- ggplot(sept_cits, aes(x = `Month with Day`, y = `Tot Day Cits`)) +
#  geom_bar(color = "red", fill = "red", stat = "identity") + 
#  theme(axis.text.x = element_text(hjust = 1),
#        plot.title = element_text(hjust = 0.5), legend.position = "bottom") +
#  ggtitle("Citations for September 2018") +
#  labs(x = 'Month and Day', y = "# of Citations")
#print(e)

# Can't plot all the data, so sampled from it.
#set.seed(12345)
#gc()
#samp_dat <- sample_n(coord_dat, 17000)
#samp_coords <- as.data.frame(table(samp_dat$New_Latitude,
#                                   samp_dat$New_Longitude))
#names(samp_coords) <- c('lat', 'long', 'Frequency')
#samp_coords$long <- as.numeric(as.character(samp_coords$long))
#samp_coords$lat <- as.numeric(as.character(samp_coords$lat))
# Remove coordination combinations where there are no tickets.
#samp_coords <- subset(samp_coords, Frequency > 0)

#tot_cits_month <- clean_data %>%
#  group_by(Month) %>%
#  summarise(Total_Month_Cits = n()) %>%
  # Some entires were missing the an issue date.
#  filter(!is.na(Month))
#graph_cits(tot_cits_month, "Citations for 2018", "Month", "Total_Month_Cits")
#lubridate::date(clean_data$TOD) <- "2018-01-01"
```

