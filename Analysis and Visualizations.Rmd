---
title: "Analysis and Visualizations"
author: "Brandon Thoma"
date: "March 22, 2019"
output: html_document
---

# What are we interested in?

1. Citations Across the Year / Month / Day.
2. Locations of the Most Citations
3. Types of Top Violations Over The Day

# Libraries

```{r, warning = FALSE}
# Import Libraries
x <- c("dplyr", "magrittr", "lubridate", "ggplot2", "fasttime", "viridis",
       "scales", "maps", "ggmap", "tidyr", "data.table", "gridExtra",
       "stringr")
lapply(x, library, character.only = TRUE)
source("Custom Created Functions.R")
rm(x)
```

# The Data

```{r}
clean_data <- fread("Cleaned Data.csv", sep = ",")
```

# Fix Date-Time Columns Back Into Date-Time Values

```{r}
clean_data$Issue_DT <- fastPOSIXct(clean_data$Issue_DT, tz = "UTC")
tz(clean_data$Issue_DT) <- "America/Los_Angeles"
clean_data$TOD <- fastPOSIXct(clean_data$TOD, tz = "UTC")
tz(clean_data$TOD) <- "America/Los_Angeles"
clean_data$Time <- format(clean_data$TOD, "%H:%M:%S")
clean_data$Hour <- format(clean_data$TOD, "%H")
```

# Tickets by Month

```{r}
tot_cits_month <- clean_data %>%
  # Removed some entires that were missing an issue date, since they were 
  # missing an issue date and time for the citation.
  filter(!is.na(Issue_DT)) %>%
  group_by(Month) %>%
  summarise(Total_Month_Cits = n())
# Reorder the order of the months.
tot_cits_month$Month <- factor(tot_cits_month$Month,
                               levels = c("Jan", "Feb", "Mar", "Apr", "May",
                                          "Jun", "Jul", "Aug", "Sep", "Oct",
                                          "Nov", "Dec"))
graph_cits(tot_cits_month, "Total Citations by Month for 2018", "Month",
           "Total_Month_Cits", "Month", "# of Citations")
```

# Two Chosen Months

```{r}
cits_by_day <- clean_data %>%
  filter(!is.na(Issue_DT)) %>%
  group_by(Date) %>%
  summarise(Tot_Day_Cits = n()) %>%
  mutate(Day = day(Date))
# So that the proper ticks will show on the plot.
cits_by_day$Day <- factor(cits_by_day$Day)
cutoffs_1 <- c(as.Date("2018-04-01"), as.Date("2018-04-30"))
cutoffs_2 <- c(as.Date("2018-09-01"), as.Date("2018-09-30"))
april_cits <- cits_by_day %>%
  filter(Date >= cutoffs_1[1] & Date <= cutoffs_1[2])
sept_cits <- cits_by_day %>%
  filter(Date >= cutoffs_2[1] & Date <= cutoffs_2[2])
```

```{r}
graph_cits(april_cits, "Total Citations by Day for April 2018", "Day",
           "Tot_Day_Cits", "Day", "# of Citations")
```

```{r}
graph_cits(sept_cits, "Total Citations by Day for September 2018", "Day",
           "Tot_Day_Cits", "Day", "# of Citations")
```

# Total Number of Citations by Time of Day and Weekday

```{r}
# Removed entries that were missing Issue Date-times.
cits_by_TOD <- clean_data %>%
  filter(!is.na(Issue_DT)) %>%
  group_by(TOD, Weekday) %>%
  summarise(Total_Cits = n())
# To have the plot start at midnight on the current day and go until the next 
# day.
x_lims <- c(floor_date(min(cits_by_TOD$TOD), "day"),
            ceiling_date(max(cits_by_TOD$TOD), "day"))
ggplot(cits_by_TOD,
       aes(x = TOD, y = Total_Cits)) +
  geom_line(color = "red",
            size = 0.2) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        plot.title = element_text(hjust = 0.5),
        legend.position = "bottom") + 
  labs(x = 'Citation Time (Military Time)',
       y = "Number of Citations",
       title = "Total Citations By Weekday and Time for 2018") + 
  ylim(0, 1500) +
  facet_wrap(~ Weekday,
             ncol = 2,
             scales = "free") +
  scale_x_datetime(breaks = date_breaks("1 hour"),
                   limits = x_lims,
                   labels = date_format("%H:%M", tz = "America/Los_Angeles"),
                   expand = c(0, 0))
```

# Average Cits by Time of Day and Weekday

```{r}
avg_cits_by_TOD <- clean_data %>%
  filter(!is.na(Issue_DT)) %>%
  select(TOD, Time, Date, Weekday) %>%
  group_by(Date, Time, Weekday) %>%
  mutate(Total_Cits = n()) %>%
  group_by(Time, Weekday) %>%
  mutate(Mean_Cits = mean(Total_Cits)) %>%
  select(TOD, Time, Weekday, Mean_Cits)
```

```{r}
x_lims <- c(floor_date(min(avg_cits_by_TOD$TOD), "day"),
            ceiling_date(max(avg_cits_by_TOD$TOD), "day"))
ggplot(avg_cits_by_TOD, aes(x = TOD, y = Mean_Cits)) +
  geom_line(color = "red", size = 0.2) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        plot.title = element_text(hjust = 0.5), legend.position = "bottom") + 
  labs(x = 'Citation Time (Military Time)', y = "Average # of Citations",
       title = "Average Number of Citations By Weekday and Time for 2018") + 
  ylim(0, 30) +
  facet_wrap(~ Weekday, ncol = 2, scales = "free") +
  scale_x_datetime(breaks = date_breaks("1 hour"), limits = x_lims,
                   labels = date_format("%H:%M", tz = "America/Los_Angeles"),
                   expand = c(0, 0))
```

# Violation Descriptions by Hour

```{r}
# Arbitrarily looked only at the top 7 since otherwise the graphic gets too
# hard to read.
top_violations <- clean_data %>%
  filter(!is.na(Issue_DT) & !(Violation_Description == "")) %>%
  group_by(Violation_Description) %>%
  summarise(Total_Cits = n()) %>%
  arrange(desc(Total_Cits)) %>%
  top_n(7, Total_Cits) %>%
  .$Violation_Description

cits_type_by_hr <- clean_data %>%
  filter(!is.na(Issue_DT) & (Violation_Description %in% top_violations)) %>%
  group_by(Hour, Violation_Description) %>%
  summarise(Total_Cits = n())

ggplot(cits_type_by_hr, aes(x = Hour, y = Total_Cits,
                            fill = Violation_Description)) + 
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(hjust = 0.5),
        plot.title = element_text(hjust = 0.5)) +
  labs(title = paste("Share of Total Citations Violation Descriptions By",
                     "Hour for 2018", ""),
       x = "Hour of the Day (Military Time)", y = "Total # of Citations",
       fill = "Type of Violation")
```

# Violation Descriptions by Car

```{r}
top_cars <- clean_data %>%
  filter(!(Violation_Description == "")) %>%
  group_by(Make) %>%
  summarise(Total_Cits = n()) %>%
  arrange(desc(Total_Cits)) %>%
  top_n(8, Total_Cits) %>%
  .$Make

cits_type_by_car <- clean_data %>%
  filter(!(Violation_Description == "") & (Make %in% top_cars) &
           (Violation_Description %in% top_violations)) %>%
  group_by(Make, Violation_Description) %>%
  summarise(Total_Cits = n())

cits_type_by_car$Make <- factor(cits_type_by_car$Make, 
                                levels = c("HYUN", "MERZ", "BMW", "CHEV",
                                           "NISS", "FORD", "HOND", "TOYT"))
# To remove x-axis scientific notation on the plot.
options(scipen = 10000)
ggplot(cits_type_by_car, aes(x = Make, y = Total_Cits,
       fill = Violation_Description)) + 
  geom_bar(stat = "identity") +
  coord_flip() + 
  theme(axis.text.x = element_text(hjust = 0.5),
        plot.title = element_text(hjust = 0.5)) +
  labs(title = "Most Common Type of Violation By Make for 2018",
       x = "Car Make", y = "Total # of Citations", fill = "Type of Violation")

# Could test difference in numbers to see if correlation between them, but from
# inspection it looks like no.
```

# Lets Look at Some Heatmaps!

```{r}
# Set Google API Key for map calls. The below is a fake key to protect my key.
# Additionally, some enabling of geocoding on GoogleAPIs account is necessary.

# Use this line of code for registering the API key for google cloud:
# register_google(key = "Your Key Here")

# 11% of the 2018 data was missing Latitude and Longitude data (the values
# were stored as 99999). These values were also transformed in the new
# coordinate system. Removed these values for the mapping data.
coord_data <- clean_data %>%
  filter(New_Latitude >= 30 & New_Longitude >= -125)

# Look at all of Los Angeles first.
la <- get_map(location = "los angeles", zoom = 10)

ggmap(la) + 
  coord_cartesian() + 
  geom_hex(data = coord_data, aes(x = New_Longitude, y = New_Latitude),
           alpha = 0.5, color = "black", show.legend = TRUE, stat = "binhex") +
  scale_fill_gradientn(colours = c("blue","red")) +
  labs(title = str_c('Heatmap of Parking Citations across LA for 2018'),
       x = "Longitude", y = "Latitude", fill = str_c('# of', '\nCitations')) +
  theme(text = element_text(color = "#444444"),
        plot.title = element_text(size = 16, face = 'bold', hjust = 0.5))
```

## Let's Look at Downtown LA More Closely

```{r}
# Note, the concentration and gradients for the zoomed in maps reflect only
# the citations that are located in that new map. 
# Limits for the zoomed in for Downtown LA.
dt_lat_lims <- c(34.03, 34.07)
dt_lon_lims <- c(-118.3, -118.21)
dt <- get_map(location = c(lon = mean(dt_lon_lims), lat = mean(dt_lat_lims)),
              zoom = 15L)
dt_coords <- zoom_in(coord_data, dt, "New_Latitude", "New_Longitude")
make_map(dt_coords, dt, "New_Latitude", "New_Longitude",
         "Heatmap of 2018 Downtown LA Parking Citations")
```

## Let's Look at the UCLA/Westwood Area More Closely

```{r}
# Limits for the zoomed in for UCLA/Westwood Area.
ucla_lat_lims <- c(34.0, 34.1)
ucla_lon_lims <- c(-118.47, -118.4)
ucla <- get_map(location = c(lon = mean(ucla_lon_lims),
                             lat = mean(ucla_lat_lims)),
                zoom = 14L)
ucla_coords <- zoom_in(coord_data, ucla, "New_Latitude", "New_Longitude")
make_map(ucla_coords, ucla, "New_Latitude", "New_Longitude",
         "Heatmap of 2018 Westwood Parking Citations")
```

## Let's Look at Venice More Closely

```{r}
# Limits for the zoomed in for Venice.
vn_lat_lims <- c(33.94, 34.03)
vn_lon_lims <- c(-118.52, -118.42)
vn <- get_map(location = c(lon = mean(vn_lon_lims), lat = mean(vn_lat_lims)),
              zoom = 14L)
vn_coords <- zoom_in(coord_data, vn, "New_Latitude", "New_Longitude")
make_map(vn_coords, vn, "New_Latitude", "New_Longitude",
         "Heatmap of 2018 Venice Parking Citations")
```

# Most Dangerous Meters and Their Locations

```{r, eval = FALSE}
# Some meters have incorrect latitude and longitude coordinates given. Choose
# median of the coordinates for lat and lon to give an approximate location of
# the meter, assuming that there are not many incorrect coordinates given.
tot_meter_cits <- clean_data %>%
  filter(!(Meter_Id == "") & !(Time_Limit == "")) %>%
  select(Meter_Id, New_Latitude, New_Longitude, Time_Limit) %>%
  group_by(Meter_Id) %>%
  mutate(Total_Cits = n()) %>%
  group_by(Meter_Id) %>%
  mutate(Approx_Lat = median(New_Latitude),
         Approx_Lon = median(New_Longitude)) %>%
  distinct(Meter_Id, Time_Limit, Total_Cits, Approx_Lat, Approx_Lon) %>%
  arrange(desc(Total_Cits))

top_outliers <- quantile(tot_meter_cits$Total_Cits, .95)

ggplot(data = tot_meter_cits, aes(Total_Cits)) +
  geom_bar(fill = "red") +
  labs(title = "# of Meters with a Given Amount of Total Citations in 2018",
       x = "# of Total Citations of the Meter", y = "Total # of Meters") +
  theme(text = element_text(color = "#444444"),
        plot.title = element_text(size = 16, face = 'bold', hjust = 0.5)) +
  geom_vline(xintercept = median(tot_meter_cits$Total_Cits), color = "black") +
  geom_text(aes(x = median(tot_meter_cits$Total_Cits) + 8, y = 1800,
                label = paste("Median of",
                              round(median(tot_meter_cits$Total_Cits), 2), "")),
            colour = "black") +  
  geom_vline(xintercept = top_outliers, color = "blue") +
  geom_text(aes(x = top_outliers + 12, y = 1800,
                label = paste(unname(top_outliers), "(95th Percentile)", "")),
            colour = "blue")

# Look at only the top 5% of meters, which are obviously outliers among all
# the meters.
worst_meters <- tot_meter_cits %>%
  filter(Total_Cits >= unname(quantile(tot_meter_cits$Total_Cits, 0.95))
# Reorder the legend in the plot.
worst_meters$Time_Limit <- factor(worst_meters$Time_Limit,
                                  levels = c("4HR", "2HR", "1HR", "30MIN",
                                             "15MIN"))
ggmap(la) + 
  coord_cartesian() + 
  geom_point(data = worst_meters, aes(x = Approx_Lon, y = Approx_Lat,
                                      color = Time_Limit),
             alpha = 1, size = 2)
```

# Extra Code

```{r, eval = FALSE}
#fines_area <- coord_data %>%
#  select("Fine_Amount", "New_Latitude", "New_Longitude")  
avg_area_fines <- coord_data %>%
  select("Fine_Amount", "New_Latitude", "New_Longitude") %>%
  group_by(New_Latitude, New_Longitude) %>%
  summarise(Avg_Fine = mean(Fine_Amount))

highest_area_fines <- coord_data %>%
  select("Fine_Amount", "New_Latitude", "New_Longitude") %>%
  group_by(New_Latitude, New_Longitude) %>%
  summarise(Max_Fine = max(Fine_Amount))

ggmap(la) %+% avg_area_fines +
  aes(x = New_Longitude, y = New_Latitude, z = Avg_Fine) + 
  stat_summary_2d(fun = mean, binwidth = c(.01, .01), alpha = 0.5) + 
  scale_fill_viridis(option = 'inferno') +
  labs(x = "Longitude", y = "Latitude") + 
  coord_map()

make_avg_fines <- clean_data %>%
  group_by(Make) %>%
  summarise(Count = n(), Avg_Make_Fine = mean(Fine_Amount, na.rm = TRUE)) %>%
  # Some citations did not include vehicle make, so removed now.
  filter(!(Make == "")) %>%
  arrange(desc(Avg_Make_Fine))

#ggmap(la) +
#  stat_density2d(data = cit_coords, aes(x = New_Longitude, y = New_Latitude,
#                                        fill = ..level.., alpha = ..level..),
#                 geom = "polygon", size = 0.01, bins = 16) +
#  scale_fill_viridis(option = 'inferno') +
#  scale_alpha(range = c(0, 0.4), guide = FALSE)

#ggmap(la) +
#  stat_density2d(data = cit_coords, aes(x = long, y = lat, fill = ..density..),
#                 geom = "tile", contour = FALSE, alpha = 0.5) +
#  scale_fill_viridis(option = 'inferno') +
#  labs(title = str_c('Heatmap of Parking Citations across LA'),
#       fill = str_c('Density Gradient of', '\nCitations')) +
#  theme(text = element_text(color = "#444444"),
#        plot.title = element_text(size = 16, face = 'bold'),
#        axis.title = element_blank())

# Need to sample since can't plot all of the coordinates
#set.seed(12345)
#ggmap(la) +
#  stat_density2d(data = sample_n(coord_data[, c("New_Latitude",
#                                                "New_Longitude")], 1200000),
#                 aes(x = New_Longitude, y = New_Latitude, fill = ..density..),
#                 geom = "tile", contour = FALSE) +
#  scale_alpha(range = c(0, 0.5)) +
#  scale_fill_viridis(option = 'inferno') +
#  labs(title = str_c('Heatmap of Parking Citations across LA'),
#       fill = str_c('Density of', '\nCitations')) +
#  theme(text = element_text(color = "#444444"),
#        plot.title = element_text(size = 16, face = 'bold'),
#        axis.title = element_blank())


#a <- coord_data %>%
#  select(New_Latitude, New_Longitude) %>%
#  filter(New_Latitude >= dt_lat_lims[1] & New_Latitude <= dt_lat_lims[2] &
#         New_Longitude >= dt_long_lims[1] & New_Longitude <= dt_long_lims[2])


#dt <- get_map(location = c(lon = mean(dt_long_lims), lat = mean(dt_lat_lims)),
#              zoom = 15L)
#ggmap(dt) +
#  stat_density2d(data = a, aes(x = New_Longitude, y = New_Latitude,
#                               fill = ..level.., alpha = ..level..),
#                 geom = "polygon", size = 0.010, bins = 40) +
#  scale_alpha(range = c(0, 0.4), guide = FALSE) +
#  scale_fill_viridis(option = 'inferno')

#ucla <- get_map(location = c(lon = mean(ucla_long_lims), lat = mean(ucla_lat_lims)),
#              zoom = 14L)
#b <- cit_coords %>%
#  filter(New_Latitude >= ucla_lat_lims[1] & New_Latitude <= ucla_lat_lims[2] &
#         New_Longitude >= ucla_long_lims[1] & New_Longitude <= ucla_long_lims[2])
#ggmap(ucla) + 
#  coord_cartesian() + 
#  geom_hex(data = b, aes(x = New_Longitude, y = New_Latitude,
#                         colour = Frequency), 
#           alpha = 0.5, color = "black", show.legend = TRUE, stat = "binhex") +
#  guides(fill = FALSE, alpha = FALSE) +
#  scale_fill_gradientn(colours = c("black","red"))

#ggmap(ucla) +
#  stat_density2d(data = b, aes(x = New_Longitude, y = New_Latitude,
#                               fill = ..level.., alpha = ..level..),
#                 geom = "polygon", size = 0.1, bins = 30) +
#  scale_alpha(range = c(0, 1), guide = FALSE) +
#  scale_fill_gradientn(colours = c("yellow","red"))
#b <- coord_data %>%
#  select(New_Latitude, New_Longitude) %>%
#  filter(New_Latitude >= ucla_lat_lims[1] & New_Latitude <= ucla_lat_lims[2] &
#         New_Longitude >= ucla_long_lims[1] & New_Longitude <= ucla_long_lims[2])

#ggmap(ucla) +
#  stat_density2d(data = ucla_coords, aes(x = New_Longitude, y = New_Latitude,
#                                         fill = ..density..),
#                 geom = 'tile', contour = FALSE, alpha = .4) +
#  scale_fill_viridis(option = 'inferno') +
#  labs(title = str_c('Heatmap of 2018 Westwood Parking Citations'),
#       fill = str_c('Number of', '\nCitations'), x = "Longitude",  y = "Latitude") +
#  theme(text = element_text(color = "#444444"),
#        plot.title = element_text(size = 16, face = 'bold', hjust = 0.5))
#

#make_map(cit_coords, "lat", "long", dt_lat_lims, dt_long_lims, 15L)

#a <- coord_data %>%
#  select(New_Latitude, New_Longitude) %>%
#  filter(New_Latitude >= dt_lat_lims[1] & New_Latitude <= dt_lat_lims[2] &
#         New_Longitude >= dt_long_lims[1] & New_Longitude <= dt_long_lims[2])


#dt <- get_map(location = c(lon = mean(dt_long_lims), lat = mean(dt_lat_lims)),
#              zoom = 15L)
#ggmap(dt) +
#  stat_density2d(data = a, aes(x = New_Longitude, y = New_Latitude,
#                               fill = ..level.., alpha = ..level..),
#                 geom = "polygon", size = 0.010, bins = 40) +
#  scale_alpha(range = c(0, 0.4), guide = FALSE) +
#  scale_fill_viridis(option = 'inferno')

#ggmap(dt) +
#  stat_density2d(data = dt_coords, aes(x = New_Longitude, y = New_Latitude,
#                                       fill = ..density..),
#                 geom = 'tile', contour = FALSE, alpha = .4) +
#  scale_fill_viridis(option = 'inferno') +
#  labs(title = str_c('Heatmap of Downtown LA Parking Citations for 2018'),
#       fill = str_c('# of', '\nCitations'), x = "Longitude",  y = "Latitude") +
#  theme(text = element_text(color = "#444444"),
#        plot.title = element_text(size = 16, face = 'bold', hjust = 0.5))


#ggmap(la) + 
#  coord_cartesian() + 
#  geom_hex(data = cit_coords, aes(x = New_Longitude, y = New_Latitude), 
#           alpha = 0.5, color = "black", show.legend = TRUE, stat = "binhex",
#           bins = 50) +
#  scale_fill_gradientn(colours = c("blue","red")) +
#  labs(title = str_c('Heatmap of Parking Citations across LA for 2018'),
#       fill = str_c('# of', '\nCitations'), x = "Longitude", y = "Latitude") +
#  theme(text = element_text(color = "#444444"),
#        plot.title = element_text(size = 16, face = 'bold', hjust = 0.5))

# The following ensures that all the coordinates in the dataset are within
# the zoomed in map.
#dt_map_lat_lims <- c(unname(unlist(attr(dt, "bb")["ll.lat"])),
#                     unname(unlist(attr(dt, "bb")["ur.lat"])))
#dt_map_lon_lims <- c(unname(unlist(attr(dt, "bb")["ll.lon"])),
#                     unname(unlist(attr(dt, "bb")["ur.lon"])))  
#dt_coords <- coord_data %>%
#  select(New_Latitude, New_Longitude) %>%
#  filter(New_Latitude >= dt_map_lat_lims[1] & 
#         New_Latitude <= dt_map_lat_lims[2] &
#         New_Longitude >= dt_map_lon_lims[1] &
#         New_Longitude <= dt_map_lon_lims[2])

#vn_map_lat_lims <- c(unname(unlist(attr(vn, "bb")["ll.lat"])),
#                     unname(unlist(attr(vn, "bb")["ur.lat"])))
#vn_map_lon_lims <- c(unname(unlist(attr(vn, "bb")["ll.lon"])),
#                     unname(unlist(attr(vn, "bb")["ur.lon"])))  
#vn_coords <- coord_data %>%
#  select(New_Latitude, New_Longitude) %>%
#  filter(New_Latitude >= vn_map_lat_lims[1] & 
#         New_Latitude <= vn_map_lat_lims[2] &
#         New_Longitude >= vn_map_lon_lims[1] &
#         New_Longitude <= vn_map_lon_lims[2])

#ucla_map_lat_lims <- c(unname(unlist(attr(ucla, "bb")["ll.lat"])),
#                     unname(unlist(attr(ucla, "bb")["ur.lat"])))
#ucla_map_lon_lims <- c(unname(unlist(attr(ucla, "bb")["ll.lon"])),
#                     unname(unlist(attr(ucla, "bb")["ur.lon"])))  
#ucla_coords <- coord_data %>%
#  select(New_Latitude, New_Longitude) %>%
#  filter(New_Latitude >= ucla_map_lat_lims[1] & 
#         New_Latitude <= ucla_map_lat_lims[2] &
#         New_Longitude >= ucla_map_lon_lims[1] &
#         New_Longitude <= ucla_map_lon_lims[2])
#ucla_coords <- coord_data %>%
#  select(New_Latitude, New_Longitude) %>%
#  filter(New_Latitude >= ucla_lat_lims[1] & New_Latitude <= ucla_lat_lims[2] &
#         New_Longitude >= ucla_long_lims[1] &
#         New_Longitude <= ucla_long_lims[2])

avg_cits_by_TOD <- clean_data %>%
  filter(!is.na(Issue_DT)) %>%
  group_by(TOD, Weekday) %>%
  summarise(Total_Cits = n()) %>%
  group_by(TOD) %>%
  mutate(Avg_Cits = mean(Total_Cits))

ggplot(avg_cits_by_TOD,
       aes(x = TOD, y = Avg_Cits)) +
  geom_line(color = "red",
            size = 0.2) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        plot.title = element_text(hjust = 0.5),
        legend.position = "bottom") + 
  labs(x = 'Citation Time (Military Time)',
       y = "Number of Citations",
       title = "Average Citations By Time By Day for 2018") + 
  scale_x_datetime(breaks = date_breaks("1 hour"),
                   limits = x_lims,
                   labels = date_format("%H:%M", tz = "America/Los_Angeles"),
                   expand = c(0, 0))

#x_lims <- c(floor_date(min(cits_type_by_TOD$TOD), "day"),
#            ceiling_date(max(cits_type_by_TOD$TOD), "day"))
ggplot(cits_type_by_TOD,
       aes(x = TOD, y = Total_Cits, colour = Weekday)) +
  geom_point(#color = "red",
            size = 0.1) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        plot.title = element_text(hjust = 0.5),
        legend.position = "bottom") + 
  labs(x = 'Citation Time (Military Time)',
       y = "# of Citations",
       title = "Total Citations By Weekday and Time for 2018") + 
  #ylim(0, 100) +
  facet_wrap(~ Violation_Description,
             ncol = 3,
             scales = "free") +
  scale_x_datetime(breaks = date_breaks("1 hour"),
                   limits = x_lims,
                   labels = date_format("%H:%M", tz = "America/Los_Angeles"),
                   expand = c(0, 0))

meter_cits <- clean_data %>%
  filter(!(Meter_Id == "") & !is.na(Time_Limit) & !(Time_Limit == ""))

a <- meter_cits %>%
  group_by(Make) %>%
  summarise(Total_Meter_Cits = n()) %>%
  filter(!(Make == "")) %>%
  mutate(Percent_Of_Meter_Cits = (round(Total_Meter_Cits / sum(Total_Meter_Cits) * 100, 2))) %>%
  arrange(desc(Percent_Of_Meter_Cits))

b <- clean_data %>%
  group_by(Make) %>%
  summarise(Total_Cits = n()) %>%
  filter(!(Make == "")) %>%
  mutate(Percent_Of_All_Cits = (round(Total_Cits / sum(Total_Cits) * 100, 2))) %>%
  arrange(desc(Percent_Of_All_Cits))

c <- a %>%
  left_join(b, by = "Make")

cor(c$Percent_Of_Meter_Cits, c$Percent_Of_All_Cits)
```
